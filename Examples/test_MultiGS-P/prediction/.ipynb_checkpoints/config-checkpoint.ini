[General]
seed = 42
threads = 10
n_replicates = 1
n_folds = 5
R_path = /isilon/ottawa-rdc/users/youf/backup_data/sharedTool/mmoe_env/bin/R
results_dir = results

[Data]
vcf_path = ../../inputFile/train_genotype.vcf
phenotype_path = ../../inputFile/train_phenotype.txt

pca_variance_explained = 0.95  # Use enough components to explain 95 percent variance
pheno_normalization = standard
genotype_normalization = standard

# For prediction mode, this section should be commented
test_vcf_path = ../../inputFile/test_genotype.vcf
test_phenotype_path = ../../inputFile/test_phenotype.txt

[Tool]
rtm-gwas-snpldb_path = ../../../Pipelines/Utilities/rtm-gwas/rtm-gwas-snpldb

[FeatureView]
feature_view = HAP      # SNP,HAP,PC:  one, any two, or all three separated by ','

[Models]
R_RRBLUP = true
R_GBLUP = true
RRBLUP = true
DeepResBLUP = true
DeepBLUP = true
ElasticNet = true
RFR = true
BRR = true
XGBoost = true
LightGBM = true
MLPGS = true
DNNGS = true
GraphConvGS = true
GraphAttnGS = true
GraphSAGEGS = true
GraphFormer = true
EnsembleGS = true


[Hyperparameters_R_RRBLUP]
method = REML    #REML|ML

[Hyperparameters_R_GBLUP] 
method = REML    #REML|ML

[Hyperparameters_RRBLUP]
lambda_value = None
method = mixed_model
lambda_method = auto    # Options: auto|reml|heritability|fixed
tol = 1e-8

[Hyperparameters_ElasticNet]
# Reduce regularization for ElasticNet: fro, 1 to 0.1->0.01->0.001
alpha = 1.0
l1_ratio = 0.1   # toward ridge: from 0.5 to 0.1-0.3

[Hyperparameters_LASSO]
alpha = 1.0

[Hyperparameters_XGBoost]
n_estimators = 100
max_depth = 6
learning_rate = 0.1
subsample = 0.8
colsample_bytree = 0.8
random_state = 42

[Hyperparameters_LightGBM]
n_estimators = 100
max_depth = -1
learning_rate = 0.1
num_leaves = 31
subsample = 0.8
colsample_bytree = 0.8
random_state = 42

[Hyperparameters_MLPGS]
hidden_layers = 1024,512,256
activation = gelu
norm = layer
residual = true
input_dropout = 0.05
dropout = 0.5

learning_rate = 0.0005
weight_decay = 0.0015
batch_size = 16
epochs = 300
early_stopping_patience = 20
warmup_ratio = 0.1
grad_clip = 1.0
seeds = 3

use_huber = true
huber_delta = 1.0

swa = true
swa_start = 0.7
swa_freq = 1

[Hyperparameters_DNNGS]
hidden_layers = 512,256,128,64
learning_rate = 0.001
batch_size = 32
epochs = 300
dropout = 0.3
activation = gelu
batch_norm = true
weight_decay = 0.0001
input_dropout = 0.1

[Hyperparameters_GraphConvGS]
hidden_channels = 128
num_layers = 2
hidden_mlp = 128
dropout = 0.2
learning_rate = 0.0005
epochs = 500
top_k = 20
graph_method = knn
knn_metric = euclidean
patience = 20

[Hyperparameters_GraphAttnGS]
hidden_channels = 128
num_layers = 2
heads = 4
hidden_mlp = 128
dropout = 0.2
learning_rate = 0.0005
epochs = 500
top_k = 20
graph_method = knn
knn_metric = euclidean
patience = 20

[Hyperparameters_GraphSAGEGS]
hidden_channels = 128
num_layers = 2
hidden_mlp = 128
dropout = 0.2
learning_rate = 0.0005
epochs = 500
top_k = 20
graph_method = knn
knn_metric = euclidean
aggr = mean
patience = 20

[Hyperparameters_GraphFormer]
gnn_type = SAGE             # Choose: SAGE|GraphConvGS|GraphAttnGS
gnn_hidden = 128            # Output size of GNN layer
transformer_layers = 2      # Number of transformer layers
d_model = 128               # Transformer dimension
nhead = 4                   # Number of attention heads
mlp_hidden = 128            # MLPGS hidden size
learning_rate = 0.001
epochs = 500
patience = 30
dropout = 0.1
weight_decay = 0.001
graph_method = knn
knn_metric = euclidean
top_k = 30

[Hyperparameters_DeepResBLUP]
base_model = R_RRBLUP
dl_model = MLPGS        # MLPGS|DNNGS|hybrid (hybrid: marker-transformer + optional sample GNN, very slow)
dl_hidden_layers = 128,64
dl_dropout = 0.2
dl_learning_rate = 0.001
dl_batch_size = 32
dl_epochs = 100

[Hyperparameters_DeepBLUP]
rrblup_lambda = 0.001
dl_hidden_layers = 128,64,32
dropout = 0.3
activation = gelu
use_precomputed_rrblup = true
train_rrblup_layer = true
learning_rate = 0.0001
batch_size = 16
epochs = 200
weight_decay = 0.0001
use_batch_norm: true
use_residual_connections: true
   
[Hyperparameters_EnsembleGS]
# models available for stacking
# 'R_RRBLUP', 'R_GBLUP', 'RRBLUP',
# 'ElasticNet', 'RFR', 'BRR',
# 'XGBoost', 'LightGBM', 
# 'DNNGS', 'MLPGS',
# 'GraphConvGS','GraphAttnGS', 'GraphSAGEGS', 'GraphFormer', 
# 'DeepResBLUP', 'DeepBLUP',
base_models = R_RRBLUP,ElasticNet,BRR, LightGBM, MLPGS
meta_model = linear   # linear|ridge 
meta_alpha = 1.0