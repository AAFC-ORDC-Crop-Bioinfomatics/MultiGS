[Log] Writing console output to: results/gs_cross_validation_20251223_124759.log
==================================================================
MultiGS-P (1.0): GENOMIC SELECTION PIPELINE USING MACHINING LEARNING AND DEEP LEARNING MODELS
==================================================================
Mode: CROSS_VALIDATION
Enabled models: R_RRBLUP, R_GBLUP, RRBLUP, ElasticNet, RFR, BRR, XGBoost, LightGBM, DNNGS, MLPGS, GraphConvGS, GraphAttnGS, GraphSAGEGS, GraphFormer, DeepResBLUP, DeepBLUP, EnsembleGS
Feature (marker) views: ['HAP']
Results directory: results
Marker file: ../../inputFile/train_genotype.vcf
Pheno file: ../../inputFile/train_phenotype.txt
------------------------------------------------------------------
Running in CROSS-VALIDATION mode
==================================================
Running cross-validation for marker type: HAP
==================================================
[Input] Using VCF genotype file: ../../inputFile/train_genotype.vcf
Total SNPs =  2000
Total samples =  298
[RUN] ../../../Pipelines/Utilities/rtm-gwas/rtm-gwas-snpldb --vcf ../../inputFile/train_genotype.vcf --thread 10 --out results/intermediate_data/train_genotype_hap
RTM-GWAS SNPLDB 2023.0 (5 Aug 2023)
INFO: reading genotype file ...
INFO: 298 individuals, 2000 loci
INFO: 23 blocks found on Chr1
INFO: 19 blocks found on Chr10
INFO: 5 blocks found on Chr11
INFO: 12 blocks found on Chr12
INFO: 37 blocks found on Chr13
INFO: 21 blocks found on Chr14
INFO: 14 blocks found on Chr15
INFO: 22 blocks found on Chr2
INFO: 17 blocks found on Chr3
INFO: 6 blocks found on Chr4
INFO: 26 blocks found on Chr5
INFO: 19 blocks found on Chr6
INFO: 16 blocks found on Chr7
INFO: 12 blocks found on Chr8
INFO: 20 blocks found on Chr9
RTM-GWAS SNPLDB has finished

haplotype vcf file: results/intermediate_data/train_genotype_hap.vcf
Total samples in haplotype vcf : 298
Total markers in haplotype vcf: 1607
Total haplotypes =  1607
Total samples =  298

[DATA] Total samples: 293 
[DATA] Feature view: HAP 
[DATA] No. markers: 1607
[DATA] Traits: ['YLD', 'PLH']
[DATA] No. traits: 2


=== [HAP] Replicate 1/1, Fold 1/5 ===
Training models: R_RRBLUP, R_GBLUP, RRBLUP, ElasticNet, RFR, BRR, XGBoost, LightGBM, DNNGS, MLPGS, GraphConvGS, GraphAttnGS, GraphSAGEGS, GraphFormer, DeepResBLUP, DeepBLUP, EnsembleGS
[HAP] Training R_RRBLUP...
✓ R package 'rrBLUP' successfully loaded
R-RRBLUP: Fitting with 234 samples, 1607 markers using REML
Calling R: mixed.solve()...
R-RRBLUP fitted: Ve=0.116022, Vu=0.000244, h²=0.002
  R_RRBLUP Trait 1/2 - SUCCESS
✓ R package 'rrBLUP' successfully loaded
R-RRBLUP: Fitting with 234 samples, 1607 markers using REML
Calling R: mixed.solve()...
R-RRBLUP fitted: Ve=0.098701, Vu=0.000246, h²=0.002
  R_RRBLUP Trait 2/2 - SUCCESS
  R_RRBLUP: 2/2 traits trained successfully
[HAP] [R_RRBLUP] memory used during training: 429.92 MB
[HAP] [R_RRBLUP] training completed in 0.10 minutes (6.27 seconds)

[HAP] Training R_GBLUP...
✓ R package 'rrBLUP' successfully loaded
R-GBLUP: Fitting with 234 samples, 1607 markers
Computing kinship matrix with R's A.mat()...
Calling R: mixed.solve() with kinship...
Breeding values shape: (234,)
Marker effects calculated via pseudoinverse, shape: (1607,)
R-GBLUP fitted: Ve=0.113474, Vu=0.197429, h²=0.635
  R_GBLUP Trait 1/2 - SUCCESS
✓ R package 'rrBLUP' successfully loaded
R-GBLUP: Fitting with 234 samples, 1607 markers
Computing kinship matrix with R's A.mat()...
Calling R: mixed.solve() with kinship...
Breeding values shape: (234,)
Marker effects calculated via pseudoinverse, shape: (1607,)
R-GBLUP fitted: Ve=0.095061, Vu=0.200668, h²=0.679
  R_GBLUP Trait 2/2 - SUCCESS
  R_GBLUP: 2/2 traits trained successfully
[HAP] [R_GBLUP] memory used during training: 23.78 MB
[HAP] [R_GBLUP] training completed in 0.10 minutes (5.95 seconds)

[HAP] Training RRBLUP...
HighPerformanceRRBLUP: 234 samples, 1607 markers, method=mixed_model
REML-like lambda estimation: λ=0.001000
Using lambda: 0.001000
RRBLUP mixed model fitted: λ=0.001000
HighPerformanceRRBLUP: 234 samples, 1607 markers, method=mixed_model
REML-like lambda estimation: λ=0.001000
Using lambda: 0.001000
RRBLUP mixed model fitted: λ=0.001000
[HAP] [RRBLUP] memory used during training: -0.25 MB
[HAP] [RRBLUP] training completed in 0.00 minutes (0.21 seconds)

[HAP] Training ElasticNet...
[HAP] [ElasticNet] memory used during training: 0.00 MB
[HAP] [ElasticNet] training completed in 0.00 minutes (0.03 seconds)

[HAP] Training RFR...
[HAP] [RFR] memory used during training: 1.41 MB
[HAP] [RFR] training completed in 0.01 minutes (0.33 seconds)

[HAP] Training BRR...
Using alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06
Using alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06
[HAP] [BRR] memory used during training: 35.62 MB
[HAP] [BRR] training completed in 0.01 minutes (0.52 seconds)

[HAP] Training XGBoost...
[HAP] [XGBoost] memory used during training: 7.44 MB
[HAP] [XGBoost] training completed in 0.02 minutes (0.94 seconds)

[HAP] Training LightGBM...
[HAP] [LightGBM] memory used during training: 7.53 MB
[HAP] [LightGBM] training completed in 0.01 minutes (0.53 seconds)

[HAP] Training DNNGS...
    [DNNGS] Epoch 1/300, LR: 0.000033, Train Loss: 1.9409, Val Loss: 0.7296
    [DNNGS] Epoch 11/300, LR: 0.000367, Train Loss: 0.6287, Val Loss: 0.2795
    [DNNGS] Epoch 21/300, LR: 0.000700, Train Loss: 0.3319, Val Loss: 0.2545
    [DNNGS] Epoch 31/300, LR: 0.000996, Train Loss: 0.2262, Val Loss: 0.2572
    [DNNGS] Epoch 41/300, LR: 0.000959, Train Loss: 0.1850, Val Loss: 0.2438
    [DNNGS] Epoch 51/300, LR: 0.000922, Train Loss: 0.1427, Val Loss: 0.2166
    [DNNGS] Epoch 61/300, LR: 0.000885, Train Loss: 0.1328, Val Loss: 0.2300
    [DNNGS] Epoch 71/300, LR: 0.000848, Train Loss: 0.1223, Val Loss: 0.2320
    [DNNGS] Epoch 81/300, LR: 0.000811, Train Loss: 0.0823, Val Loss: 0.2216
    [DNNGS] Epoch 91/300, LR: 0.000774, Train Loss: 0.0848, Val Loss: 0.1914
    [DNNGS] Epoch 101/300, LR: 0.000737, Train Loss: 0.0869, Val Loss: 0.1818
    [DNNGS] Epoch 111/300, LR: 0.000700, Train Loss: 0.0679, Val Loss: 0.1817
    [DNNGS] Epoch 121/300, LR: 0.000663, Train Loss: 0.0646, Val Loss: 0.1962
    [DNNGS] Epoch 131/300, LR: 0.000626, Train Loss: 0.0768, Val Loss: 0.1964
    [DNNGS] Epoch 141/300, LR: 0.000589, Train Loss: 0.0669, Val Loss: 0.1899
    [DNNGS] Epoch 151/300, LR: 0.000552, Train Loss: 0.0650, Val Loss: 0.1950
Early stopping at epoch 153
Loaded best DNNGS model with validation loss: 0.1742
[HAP] [DNNGS] memory used during training: 4.36 MB
[HAP] [DNNGS] training completed in 3.33 minutes (199.85 seconds)

[HAP] Training MLPGS...
    [MLPGS] Epoch 1/300, Train Loss: 0.3481, Val Loss: 0.4829
    [MLPGS] Epoch 11/300, Train Loss: 0.0949, Val Loss: 0.2206
    [MLPGS] Epoch 21/300, Train Loss: 0.0610, Val Loss: 0.1462
    [MLPGS] Epoch 31/300, Train Loss: 0.0438, Val Loss: 0.1339
    [MLPGS] Epoch 41/300, Train Loss: 0.0386, Val Loss: 0.1463
    [MLPGS] Epoch 51/300, Train Loss: 0.0364, Val Loss: 0.1530
Early stopping at epoch 56
[HAP] [MLPGS] memory used during training: 37.23 MB
[HAP] [MLPGS] training completed in 3.98 minutes (238.74 seconds)

[HAP] Training GraphConvGS...
[GraphConvGS] Building sample graph with 234 samples using KNN (top_k=20, metric=euclidean)
[KNN Graph] Built graph with 7582 edges, top_k=20, metric=euclidean
[GraphConvGS] Graph built with 7582 edges
[GraphConvGS] Starting training for 500 epochs
    [GraphConvGS] Epoch 11/500 | train_loss: 0.470727 | val_loss: 0.408494
    [GraphConvGS] Epoch 21/500 | train_loss: 0.397755 | val_loss: 0.351101
    [GraphConvGS] Epoch 31/500 | train_loss: 0.376491 | val_loss: 0.323481
    [GraphConvGS] Epoch 41/500 | train_loss: 0.337641 | val_loss: 0.299964
    [GraphConvGS] Epoch 51/500 | train_loss: 0.319580 | val_loss: 0.281369
    [GraphConvGS] Epoch 61/500 | train_loss: 0.291603 | val_loss: 0.261899
    [GraphConvGS] Epoch 71/500 | train_loss: 0.279422 | val_loss: 0.246573
    [GraphConvGS] Epoch 81/500 | train_loss: 0.286683 | val_loss: 0.240973
    [GraphConvGS] Epoch 91/500 | train_loss: 0.277564 | val_loss: 0.225472
    [GraphConvGS] Epoch 101/500 | train_loss: 0.258069 | val_loss: 0.217399
    [GraphConvGS] Epoch 111/500 | train_loss: 0.243371 | val_loss: 0.213549
    [GraphConvGS] Epoch 121/500 | train_loss: 0.224626 | val_loss: 0.201815
    [GraphConvGS] Epoch 131/500 | train_loss: 0.222953 | val_loss: 0.190777
    [GraphConvGS] Epoch 141/500 | train_loss: 0.211291 | val_loss: 0.184810
    [GraphConvGS] Epoch 151/500 | train_loss: 0.220602 | val_loss: 0.178310
    [GraphConvGS] Epoch 161/500 | train_loss: 0.221091 | val_loss: 0.185906
    [GraphConvGS] Epoch 171/500 | train_loss: 0.218906 | val_loss: 0.182090
    [GraphConvGS] Epoch 181/500 | train_loss: 0.209480 | val_loss: 0.167626
    [GraphConvGS] Epoch 191/500 | train_loss: 0.204853 | val_loss: 0.165486
    [GraphConvGS] Epoch 201/500 | train_loss: 0.203158 | val_loss: 0.161774
    [GraphConvGS] Epoch 211/500 | train_loss: 0.183784 | val_loss: 0.154177
    [GraphConvGS] Epoch 221/500 | train_loss: 0.188654 | val_loss: 0.152569
    [GraphConvGS] Epoch 231/500 | train_loss: 0.189774 | val_loss: 0.152641
    [GraphConvGS] Epoch 241/500 | train_loss: 0.199384 | val_loss: 0.144010
    [GraphConvGS] Epoch 251/500 | train_loss: 0.194905 | val_loss: 0.140703
    [GraphConvGS] Epoch 261/500 | train_loss: 0.178379 | val_loss: 0.136354
    [GraphConvGS] Epoch 271/500 | train_loss: 0.174803 | val_loss: 0.133744
    [GraphConvGS] Epoch 281/500 | train_loss: 0.173173 | val_loss: 0.126782
    [GraphConvGS] Epoch 291/500 | train_loss: 0.173181 | val_loss: 0.127258
    [GraphConvGS] Epoch 301/500 | train_loss: 0.177077 | val_loss: 0.125186
    [GraphConvGS] Epoch 311/500 | train_loss: 0.162801 | val_loss: 0.123407
    [GraphConvGS] Epoch 321/500 | train_loss: 0.162223 | val_loss: 0.126635
[GraphConvGS] Early stopping at epoch 321. Best val 0.121993
[GraphConvGS] Training completed successfully
[HAP] [GraphConvGS] memory used during training: 2.71 MB
[HAP] [GraphConvGS] training completed in 0.96 minutes (57.67 seconds)

[HAP] Training GraphAttnGS...
[GraphAttnGS] Building sample graph with 234 samples using KNN (top_k=20, metric=euclidean)
[KNN Graph] Built graph with 7582 edges, top_k=20, metric=euclidean
[GraphAttnGS] Graph built with 7582 edges
[GraphAttnGS] Starting training for 500 epochs
    [GraphAttnGS] Epoch 11/500 | train_loss: 0.365468 | val_loss: 0.333383
    [GraphAttnGS] Epoch 21/500 | train_loss: 0.299594 | val_loss: 0.271910
    [GraphAttnGS] Epoch 31/500 | train_loss: 0.296030 | val_loss: 0.242020
    [GraphAttnGS] Epoch 41/500 | train_loss: 0.255159 | val_loss: 0.211907
    [GraphAttnGS] Epoch 51/500 | train_loss: 0.257657 | val_loss: 0.198819
